model: distilgpt2

seed: 9
num_workers: 8

cache_dir: data/cache_dir
revision: true
push_to_hub: false

dataset:
  name: wikitext
  config_name: wikitext-103-v1
  validation_split_percentage: 3

  tokenizer: gpt2
  use_fast_tokenizer: true

training:
  per_device_train_batch_size: 3
  per_device_eval_batch_size: 3
  
  learning_rate: 5e-5
  weight_decay: 0.0
  gradient_accumulation_steps: 8
  lr_scheduler_type: linear
  num_warmup_steps: 0

  num_train_epochs: 5
  num_steps_before_eval: 2000

  kd_loss_coef: 1
  use_distillation: False
  kernel: elu
