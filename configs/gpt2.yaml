model: distilgpt2

seed: 9
num_workers: 8

cache_dir: data/cache_dir
revision: true
push_to_hub: false
feature_map: approx

dataset:
  name: wikitext
  config_name: wikitext-103-v1
  validation_split_percentage: 3

  tokenizer: gpt2
  use_fast_tokenizer: true

training:
  per_device_train_batch_size: 48
  per_device_eval_batch_size: 48
  
  learning_rate: 1e-2
  weight_decay: 0.0
  gradient_accumulation_steps: 8
  lr_scheduler_type: linear
  num_warmup_steps: 100

  num_train_epochs: 15
  num_steps_before_eval: 50

  kd_loss_coef: 1000
  use_distillation: True
  kernel: elu

  max_seq_len: 256
